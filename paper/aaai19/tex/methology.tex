\section{Methodology}
\label{methodology}

\begin{algorithm}
	\renewcommand{\algorithmicrequire}{\textbf{Input:}}
	\renewcommand{\algorithmicensure}{\textbf{Output:}}
	\caption{SREC}
	\label{alg:1}
    \begin{algorithmic}[1]
        \REQUIRE a set of words pairs $\{(w_1, w_2)\}$
        \ENSURE a semantic relatedness score $sr \in [0,1]$
        \STATE initialize entites set ${E_{w_1}}$, ${E_{w_2}}$ empty
        \STATE initialize the minimal subgraph $graph\_ent$ that describes entites pairs $<e_1,e_2>$
        \STATE initialize the vector set $vector\_ent\_set$ that represents entites set ${E_{w_1}}$, ${E_{w_2}}$
        \STATE initialize the vector set $scores\_set$ that record multiple semantic relatedness sorces 
        \STATE ${E_{w_1}} \leftarrow query\_lookup(w_1)$; ${E_{w_2}} \leftarrow query\_lookup(w_2)$
        \FOR  {$e_1 \in E_{w_1}, e_2 \in E_{w_2} $}
        \STATE $graph\_ent \leftarrow query\_spqrql(e_1, e_2)$ (updating)
        \STATE $vector\_ent\_set \leftarrow embedding(graph\_ent)$
        \STATE $v_{e_1} \leftarrow find(e_1, vector\_ent\_set)$; $v_{e_2} \leftarrow find(e_2, vector\_ent\_set)$
        \STATE $scores\_set.append(e_1, e_2, cos(v_{e_1}, v_{e_2}))$
        \ENDFOR
        \STATE $sr \leftarrow weighted(scores\_set)$
		\STATE \textbf{return} $r$
	\end{algorithmic}  
\end{algorithm}

In order to compute semantic relatedness of a pair of words, we propose a model which is
threefold. 
For a given pair of words, we first query the corresponding entities in knowledge graph.
We need to construct a specific graph which contains all related entities and attributes between the 
corresponding entity pairs. Then we use \emph{Starspace}\cite{corr/Ledell17} to train the constructed 
graph. For each entity and relationship, this method produces a representation of vector.
In our method, we use cosine function to compare vectors corresponding to word pairs and get the relatedness scores.
Besides, in the query step, we would get several corresponding entities for an input word. Inspired by
the SenseEmbed\cite{acl/IacobacciPN15}, we combine the relatedness scores computed from the multiple pairs of entities 
as the final measure sorce between two words.

\subsection{Construct graph}
\label{sec:construct graph}
Our aim is to compute the semantic relatedness between a pair of words. The process of relatedness measure
needs complete and ample background knowledge which can be gathered from knowledge graph, such as
DBPedia\footnote{http://wiki.dbpedia.org/}, 
YAGO\footnote{https://www.mpi-inf.mpg.de/departments/databases-and-information-systems/research/yago-naga/yago} etc.
The first problem we face is how to obtain knowledge which is associated with given words from knowledge graph. 
In our model, we utilize the DBPedia as our knowledge base to gather corresponding entities triggered by the given words.
We rely on lookup service\footnote{http://lookup.dbpedia.org/api/search/KeywordSearch} that is provided by DBPedia to achieve this object. 
We use $W_{w_i}$ to denote the set of corresponding entities in knowledge graph when we complete a query by word $w_i$.
Then we get $W_{w_i}=\{E_1,...,E_k\}$ where $E_i$ is the $i_{th}$ query entity associated with the given word.
As show in Table \ref{entities}, for a given word pair \emph{(automobile,car)}, 
we can get the appropriate entities which are described as URIs in DBPedia. 
These URIs describe entities so accurate that we can access knowledge graph using powerful
query language SPARQL\footnote{https://www.w3.org/TR/sparql11-overview/} to get everything we want.

% Please add the following required packages to your document preamble:
% \usepackage{booktabs}
\begin{table*}[]
    \small
    \centering
    \caption{Query Entities (\emph{DBPedia:} equals \emph{http://dbpedia.org/resource/})}
    \label{entities}
    \renewcommand\arraystretch{1.6}
    \setlength{\tabcolsep}{0.8mm}{
    \begin{tabular}{llclc}
    \toprule
    \textbf{Words}  & automobile                                               & \textbf{Refcount} & car                                      & \textbf{Refcount} \\ \hline
    \textbf{Entity} & DBPedia:Automobile                   & 5096     & DBPedia:Automobile   & 5096     \\ \hline
    \textbf{Entity} & DBPedia:Auto\_racing                 & 2885     & DBPedia:NASCAR       & 4317     \\ \hline
    \textbf{Entity} & DBPedia:Ferry                        & 2501     & DBPedia:Tram         & 3988     \\ \hline
    \textbf{Entity} & DBPedia:Internal\_combustion\_engine & 1639     & DBPedia:Auto\_racing & 2885     \\ \hline
    \textbf{Entity} & DBPedia:Gasoline                     & 1486     & DBPedia:Ferry        & 2501     \\ \hline
    \end{tabular}    
    }
\end{table*}


% sparql query norm for graph construction

An entity can be surrounded by entities as well as attributes. There is an example to illustrate this structure
in knowledge graph. Suppose that there is a person \emph{A}, his age is \emph{24}. His name is \emph{"John A"}.
He has a friend called person \emph{B}. This person \emph{B} is an Entity surrounding the \emph{A}.
Then the number \emph{24} and the literal \emph{"John A"} are not entities in knowledge graph,
but all are the attributes surrounding this person \emph{A}.
We can use the URIs shown in table\ref{entities} to access knowledge graph with the help of SPARQL. Next we need to construct a subgraph 
that contains all related entities, attributes and relations between the corresponding entity pairs. Inspired by BabelRelate!\cite{aaai/NavigliP12},
we propose a modified method to get our semantic subgraph. We get query sets of entities $W_{w_1}$ and $W_{w_2}$ individually
by words $w_1$ and $w_2$. Then we select the subgraph which contains all the paths which connect entities in
$ENT = W_{w_1} \cup W_{w_1}$. Besides, this subgraph also contains all attributes for each entity in $ENT$. We do 
this by building a directed graph ${G = (V, E)}$ which contains all relevant information which describes
the entities set.

i) We define the set $V$ in $G$ as $V:=ENT$ first. 
The size of set $V$ is not fixed. It would be extended in the following steps.
As for the set $E$, we initialize it as empty, i.e., $E:=\emptyset$.

ii)The goal of our method is to get the precise vector representations for entities,
that requires more complete information which surround the entities.
Accordingly, for a specific entity, we not only need to consider its neighbor entities or attributes, but also
need to find all paths which connect the nodes in $V$. 
It is known to all, the shorter length of paths between two entities, the more relative these entities are.
We firstly get the one-hop neighbors for each $v \in V$. 
Secondly, we adopt Depth-First Search(DFS) to go through the knowledge graph. Every time we find a node
$v^{'} \in V$ but $v \ne v^{'}$ along a path($v, v_1, v_2,...,v_n, v^{'}$), we add all intermedia 
nodes and edges in this path to $G$, i.e., $V:=V \cup \{v_1, ..., v_n\}$, 
$E:=E \cup \{(v, v_1), ..., (v_n, v^{'})\}$.

iii) Next, we get all relevant attributes which are described as literal, number or something 
else special symbol in knowledge graph. For one entity $v_i \in V$, we collect all the one-hop attributes 
$\{a_1, a_2, ..., a_k\}$. Then we have $V:=V \cup \{a_1, ..., a_k\}$ and
$E:=E \cup \{(v_i, a_1), ..., (v_i, a_k)\}$ ($v_i \in V$).

By this way, we extract a subgraph from DBPedia which consists of the relevant information which describes
the pair of entities.

\subsection{Embedding for Subgraph}
\label{sec:train}
The constructed graph is fundamentally a multi-relational graph in which an entity is described by a set of discrete
\emph{entities} and \emph{attributes}. Fortunately, there have been an excellent work proposed by Facebook AI Research,
\emph{StarSpace}\cite{corr/Ledell17}. The model works by embedding those entities comprised of discrete features and
comparing them against each other. In this section, we introduce the basic contents of \emph{StarSpace} briefly and 
elaborate how we utilize this model to get the vector representation of entities and relations. \emph{StarSpace} is available as
an open-source project at github\footnote{https://github.com/facebookresearch/StarSpace}.

In \emph{StarSpace}, to train our model, we need to compare entities which are described by a set of discrete
\emph{entities} and \emph{attributes}. Specially, there is the following loss function in $StarSpace$:

\begin{small}
    \begin{equation}
        \nonumber
        \label{starspace_formula}
        \sum_{\substack{(a,b) \in V^+\\ b^- \in V^-}}L^{batch}(sim(a,b),sim(a,b_1^-),...,sim(a,b_k^-))
    \end{equation}
\end{small}
In our problem, the input data is a graph of $(h, r, t)$ triples, consisting of a head entity $h$, 
a relation $r$ and a tail entity $t$.
Following the original paper which describes $StarSpace$, there are several explanations for this loss function:

1) The positive entity pair (a,b) comes from the set $V^+$ sampled from constructed graph $G$. 
In our problem, the input is a group of triples $(h, r, t)$. In order to make our input fit
the sample batch $(a, b)$, we need to select uniformly at random to get positive sample $V^+$ in two strategies:
(i)$a$ consists of the bag of features $h$ and $r$, while $b$ consists only of $t$; 
(ii)$a$ consists of $h$, and $b$ consists of $r$ and $t$. 

2) Negative entities $b^-$ are sampled from the set of possible entities $V^-$.  
StarSpace utilizes a $k$-negative sampling strategy\cite{corr/Mikolov13} to select $k$ negative pairs for each batch update. 
They select randomly whithin the set of entities that can appear in the second argument of the similarity function.

3) The selection of function $sim(.,.)$ is designed as a hyper-parameter: cosine similarity and inner product.
In our problem, we adopt cosine similarity for the model as the cosine works better than inner product for
larger dataset. This situation is mentioned in the paper of StarSpace.

4) The loss function $L_{batch}$ compares the positive pairs $(a,b)$ with the negative pairs $(a, b_i^-)$, $i=1,...,k$.
It is also optional between margin ranking loss and negative log loss of softmax. All experiments in $StarSpace$ show
the former performed on par or better. Thus we use margin ranking loss as our loss function for computing semantic relatedness.

5) The method optimization inherit the strategy of stochastic gradient descent(SGD) used in $Starspace$. Each SGD step is one
sampling from $V^+$ in the outer sum.

As a result, we take the constructed graph $G$ of $(h, r, t)$ triples as inputs for the training model.
For each entity and relation in graph $G$, there is a fixed-length vector which can
then be used to compute semantic relatedness via cosine function.

%(Experiments:K-sample,dim of vector,epochs)
% \cite{aaai/BordesWCB11}


\subsection{Semantic Relatedness Measure}
\label{sec:measure}
For a given pair of words($w_m$, $w_n$), we get ($W_{w_m}$, $W_{w_n}$), and $W_{w_m}=\{E_m^1,E_m^2...,E_m^k\}$,
$W_{w_n}=\{E_n^1,E_n^2,...,E_n^k\}$. Then for each entity $E_m^i$ in $W_{w_m}$, we will get learnt embedding vector
$\overrightarrow E_m^i$.
Note that, there might be different number of entities associated with the given
word. We just consider the top-$k$ entities in each entities set. An analysis of the impact of $k$ is
given in section of experiments.

For a pair ($w_1$, $w_2$), we compute semantic relatedness between their corresponding entities. After that, we
get $k*k$ relatedness results where $k$ is number of entities queried from knowledge graph.
The task which combines multiple semantic measurement is similar with the work in SenseEmbed\cite{acl/IacobacciPN15}.
The author captureed the different meanings of a word and transformed word embedding to the sense level.
They utilized the weighted combination of comparison in sense level which achieved a high correlation coefficient.
In our work, the multiple-pair entities produce different semantic measurement. Traditional work only considers the
the relatedness measurement of closest objects. If we only utilize closest combination, the contributions of the other
entities would be ignored. Following the work in SenseEmbed, we combine all those relatedness
results reasonably to get more human-like measurement.

1) For two entities $E_1$ and $E_2$, we utilize cosine function to compute the
distance between two embedding vectors($\overrightarrow E_1$, $\overrightarrow E_2$):

\begin{small}
    \begin{equation}
        \label{cos}
        \nonumber
        Cos(\overrightarrow E_1,\overrightarrow E_2) = \frac{\overrightarrow E_1 \cdot 
        \overrightarrow E_2}{\left \| \overrightarrow E_1 \right \|\left \| \overrightarrow E_2 \right \|}
    \end{equation}
\end{small}

2) Following SenseEmbed, we take two strategies to compute semantic relatedness between given
words $w_1$ and $w_2$ for comparison. One is conventional approach \cite{BudanitskyH06} which considers just the closest entities
among multiple vector pairs. There are $W_{w_1}$ and $W_{w_2}$ represent two entities sets associated with two
input words $w_1$ and $w_2$. $\overrightarrow E_i$ is the learnt embedding vector of entity $E_i$. We can get the
formalization for this strategy.

\begin{small}
    \begin{equation}
        \label{cos}
        \nonumber
        Rel_{closest}(w_1, w_2) = \max \limits_{\substack{E_1 \in W_{w_1} \\ E_2 \in W_{w_1}}}
        Cos(\overrightarrow E_1,\overrightarrow E_2)
    \end{equation}
\end{small}
However this relatedness measurement approach misses contributions of the other entities.
In fact, psychological studies suggest that humans, while comparing similarity between a pair of words, consider
different meanings of two words but not only the closest pairs\cite{Tversky77}. This claim can be directly popularized to compute
relatedness between words. There usually are various meanings for a single word. Analogously we can query multiple
entities associated with the word from knowledge graph. Only consider the entities pair which have the closest distance
would not be conforming with the way of human thinking.

There is another strategy for computing semantic relatedness, called \emph{weighted}, in which
we consider the contributions of different entities associated with given words. The contributions are
scaled according to how they relative to the words. Fortunately, the lookup services of DBPedia can
return a list of ranked DBpedia resources for a search string or a single word. There is a specific label called
$Refcount$ that count the number of Wikipedia page inlinks for a resource. This number is required for ranking.
We can utilize this number to estimate the dominance of each specific entity resource in knowledge graph.
To this end, we use an operation of normalization. For each entity $E \in W_{w_i}$, we get the dominance of
$E$ by dividing the value of $Refcount$ by the sum of all $Refcount$ value of entities in  $W_{w_i}$:

\begin{small}
    \begin{equation}
        \label{cos}
        \nonumber
        d(E) = \frac{refcount(E)}{\sum_{{E}'\in W_i} refcount({E}')}
    \end{equation}
\end{small}

Besides, those entities which are closer would play a more important role in computing semantic relatedness.
Following the SenseEmbed, we model this by biasing the relatedness computation towards closer entities through a power function with
parameter $\alpha$. The relatedness of a pair of words $w_1$ and $w_2$ is computed by using $weighted$ strategy:

\begin{small}
    \begin{equation}
        \begin{split}
        \label{cos}
        \nonumber
        Rel_{weight}&(w_1, w_2)=\\ 
        &\sum_{E_1 \in W_1}\sum_{E_2 \in W_2}d(E_1)d(E_2)Cos(\overrightarrow E_1,\overrightarrow E_2)^\alpha 
        \end{split}
    \end{equation}
\end{small}

In this strategy, we given the entity pairs which is closer a more important role to determine the final semantic relatedness
score. Experiments in below show that the $weighted$ strategy outperforms the $closest$.
% \begin{small}
%     \begin{math}
%         \label{cos}
%         \nonumber
%         Dis(E_1,E_2)
%         =\lambda Cos(\overrightarrow E_1,\overrightarrow E_2) + (1-\lambda)\frac{1}{path(E_1, E_2)}
%     \end{math}
% \end{small}


% \begin{small}
%     \begin{equation}
%         \label{cos}
%         \nonumber
%         Cos^*(\overrightarrow E_1,\overrightarrow E_2)=
%         \begin{cases} 
%         Cos(\overrightarrow E_1,\overrightarrow E_2) \times \beta, &if(s_1, s_2) \in E\\
%         Cos(\overrightarrow E_1,\overrightarrow E_2) \times \beta^{-1}, &otherwise
%         \end{cases}
%     \end{equation}
% \end{small}