# SENSEMBED: Learning Sense Embeddings for Word and Relational Similarity

Paper:
- *Motivation*: Word embeddings have recently gained considerable popularity for modeling words 
in different Natural Language Processing(NLP) tasks including semantic similarity measurement.
However, notwithstanding their success, word embeddings are by their nature unable to capture
polysemy,*as different meanings of a word are conflated into a single representation*. In addition,
their learning process usually relies on massive corpora only, *preventing them from taking 
advantage of stuctured knowledge.*
- Keywords: embedding, semantic similarity, relational similarity, knowledge graph, entity linking
- Methods: 
    ![Overview][1]

English:
- words:
    - polysemy(一词多意)
    - conflated(合并，异文合并)
    - celebrated(著名的)
    - syntactic/semantic(句法/语法)
    - antecedent(前件，先行词)
    - contextual evidence(相关证据)
    - drawback(缺点)
    - real-valued(实数)
    - comprise(包含)
    - hereafter(将来，来世)
    - synonymous(同义词)
    - lemma(引理)
    - vicinity(邻近)
    - religious(宗教)
    - devotee(信徒)
    - analogy(类比，类推，类似)
    - transformation(变换)
    - subtracting(减法)
    - augmented(增广的)
    - notwithstanding(虽然; 尽管; 固然)
    - Time-consuming(耗时; 费时; 浪费时间的)
    - alleviate(减轻，缓和)

- sentences:
    - sth have recently gained considerable popularity for sth. 在某方面取得巨大成就  
    - by one's nature 固有的问题，因为什么的天性
    - in addition 替代 one the other hand...

[1]: https://github.com/trajepl/pando/blob/master/related/fig/sense-embedding.jpg?raw=true "sense embedding"